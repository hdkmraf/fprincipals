Plan:

- analyze the data source (page structure, api, etc.)
- create a repository
- check out scrappy
- update the plan

---

Plan:

- basic scrapper, pagination
- find all the required data on the page
- email questions
- plan update

---

Future:

- unittest scrapers (contracts)?
- write a basic scraper, get necessary data
- how to run
- declare an Item
- use ItemPipeline to save data (requires activation)?
- export (use stdout?) or just use -o, create an egg?
- review settings, what I need to modify?
- do I need to save state?